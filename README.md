This repository contains an implementation and explanation of the Transformer architecture, a state-of-the-art deep learning model based entirely on self-attention mechanisms. Unlike RNNs or LSTMs, Transformers process sequences in parallel, enabling faster training and better performance on long-range dependencies.

Transformers form the backbone of modern NLP and Generative AI models such as BERT, GPT, T5, and LLaMA, and are widely used in tasks like machine translation, text classification, summarization, question answering, and sequence modeling.

The repository focuses on:

Scaled Dot-Product Attention and Multi-Head Attention

Positional Encoding

Encoderâ€“Decoder architecture

Training and inference workflows

Practical implementation using TensorFlow / PyTorch
